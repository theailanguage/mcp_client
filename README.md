# üöÄ MCP Client with Gemini AI

[üì¢ Subscribe to The AI Language on YouTube!](https://youtube.com/@theailanguage?sub_confirmation=1)

Welcome! This project features multiple MCP clients integrated with **Google Gemini AI** to execute tasks via the **Model Context Protocol (MCP)** ‚Äî with and without LangChain.

Happy building, and don‚Äôt forget to subscribe!  


## MCP Client Options

This repository includes **four MCP client options** for various use cases:

| Option | Client Script | LangChain | Config Support | Transport | Tutorial |
|--------|-------------------------------|------------|----------------|-----------|----------|
| 1 | `client.py` | ‚ùå | ‚ùå | STDIO | [Legacy Client](https://youtu.be/GAPncIfnDwg) |
| 2 | `langchain_mcp_client.py` | ‚úÖ | ‚ùå | STDIO | [LangChain Client](https://youtu.be/hccNm88bk6w) |
| 3 | `langchain_mcp_client_wconfig.py` | ‚úÖ | ‚úÖ | STDIO | [Multi-Server](https://youtu.be/nCnBWVv2uTA) |
| 4 | `client_sse.py` | ‚ùå | ‚ùå | SSE (Loca & Web) | [SSE Client](https://youtu.be/s0YJNcT1XMA) |

If you want to add or reuse MCP Servers, check out [the MCP Servers repo](https://github.com/modelcontextprotocol/servers).

---

## ‚ú™ Features

‚úÖ Connects to an MCP server (STDIO or SSE)  
‚úÖ Uses **Google Gemini AI** to interpret user prompts  
‚úÖ Allows **Gemini to call MCP tools** via server  
‚úÖ Executes tool commands and returns results  
‚úÖ (Upcoming) Maintains context and history for conversations  

---

### Running the MCP Client

Choose the appropriate command for your preferred client:

- **Legacy STDIO** ‚Äî `uv run client.py path/to/server.py`
- **LangChain STDIO** ‚Äî `uv run langchain_mcp_client.py path/to/server.py`
- **LangChain Multi-Server STDIO** ‚Äî `uv run langchain_mcp_client_wconfig.py path/to/config.json`
- **SSE Client** ‚Äî `uv run client_sse.py sse_server_url`

### Project Structure

```
mcp-client-gemini/
‚îú‚îÄ‚îÄ client.py                        # Basic client (STDIO)
‚îú‚îÄ‚îÄ langchain_mcp_client.py         # LangChain + Gemini
‚îú‚îÄ‚îÄ langchain_mcp_client_wconfig.py # LangChain + config.json (multi-server)
‚îú‚îÄ‚îÄ client_sse.py                   # SSE transport client (local or remote)
‚îú‚îÄ‚îÄ .env                            # API key environment file
‚îú‚îÄ‚îÄ README.md                       # Project documentation
‚îú‚îÄ‚îÄ requirements.txt                # Dependency list
‚îú‚îÄ‚îÄ .gitignore                      # Git ignore rules
‚îú‚îÄ‚îÄ LICENSE                         # License information
```

### How It Works

1. You send a prompt:
   > Create a file named test.txt
2. The prompt is sent to **Google Gemini AI**.
3. Gemini uses available **MCP tools** to determine a response.
4. The selected tool is executed on the **connected server**.
5. The AI returns results and maintains **conversation context** (if supported).

## ü§ù Contributing

At this time, this project does **not accept external code contributions**.

This is to keep licensing simple and avoid any shared copyright.

You're very welcome to:
‚úÖ Report bugs or request features (via GitHub Issues)  
‚úÖ Fork the repo and build your own version  
‚úÖ Suggest documentation improvements

If you'd like to collaborate in another way, feel free to open a discussion!

